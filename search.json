[{"title":"Doris BE存储结构","url":"/2021/02/27/Doris BE存储结构/","content":"\n\n# Doris Tablet\n\n## BE目录结构\n存储根路径：\n\nstorage_root_path = /data/doris/data/data\n\n\n```bash\ncd /data/doris/data/data\n// 查找tablet id为67811的目录\ntree -f | grep 67811\n```\n\n```\n|   `-- ./492/67811\n|       `-- ./492/67811/587730488\n|           |-- ./492/67811/587730488/0200000000002f39714b25d3aab31dd603b0eaf176e74e93_0.dat\n|           `-- ./492/67811/587730488/67811.hdr\n./492/${TabletId}/${SchemaHash}/${RowSetId}_${SegmentId}.dat\n```\n\n```sql\nshow tablet 67811;\nSHOW PROC '/dbs/67740/67742/partitions/67744/67743/67811';\n/dbs/${DbId}/${TableId}/partitions/${PartitionId}/${IndexId}/${TabletId}\n```\n\n```bash\ncd /data/doris/data/data\ntree -f | grep 67811\n```\n\n```\nSHOW PROC '/dbs/67740/67742/partitions/67744/67743/67811';\n/dbs/${DbId}/${TableId}/partitions/${PartitionId}/${IndexId}/${TabletId}\n\n|   `-- ./492/67811\n|       `-- ./492/67811/587730488\n|           |-- ./492/67811/587730488/0200000000002f39714b25d3aab31dd603b0eaf176e74e93_0.dat\n|           `-- ./492/67811/587730488/67811.hdr\n./492/${TabletId}/${SchemaHash}/${RowSetId}/${SegmentId}.dat\n```\n\n\n\n## BE元数据\n```bash\n// 查看tablet 元数据 \n// http://192.168.41.112:8039/api/meta/header/163365/1641718687\n/data/doris/be/lib/meta_tool --operation=get_meta --root_path=/data/doris/data --tablet_id=163365 --schema_hash=1641718687\n\n\n// 查看segment文件的元数据\n/data/doris/be/lib/meta_tool --operation=show_segment_footer --file=/data/doris/data/data/172/163365/1641718687/0200000000000004784b70783a5f6bc6e610d929adda15b8_0.dat\n```\n\n\n### ordinal index & short key\n\n### data model & data format & data management\ndata model: table schema, 分区， 分桶\n\ndata format: data and index format -> \n\ndata management: delta, base, cumulative -> compaction policy\n\n\n\n[1] https://doris.apache.org/master/zh-CN/sql-reference/sql-statements/Data%20Manipulation/SHOW%20TABLET.html\n[2] https://doris.apache.org/master/zh-CN/administrator-guide/operation/tablet-meta-tool.html#%E5%B1%95%E7%A4%BA-pb-%E6%A0%BC%E5%BC%8F%E7%9A%84-tabletmeta\n[3] https://doris.apache.org/master/zh-CN/administrator-guide/operation/tablet-meta-tool.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["OLAP"]},{"title":"Spark SQL - Join Types","url":"/2021/02/23/Spark SQL- join types/","content":"\n\n# Spark SQL - Join Types\n\n\n## 1. 常见的SQL JOIN\n\n![joins](/images/join_types/joins.png)\n\n\n| SQL | Name\t | JoinType |\n| ----- | ---- | ---- |\n| **CROSS** |  cross |CROSS |   \n| **INNER** | inner | INNER |\n| **FULL OUTER** | outer, full, full outer | FULL OUTER | \n| **LEFT ANTI** | left anti, anti | LEFT ANTI |  \n| **LEFT OUTER** | left outer, left | LEFT OUTER |\n| **LEFT SEMI** | left semi, semi | LEFT SEMI |\n| **RIGHT OUTER** | right outer, right | RIGHT OUTER |\n| **NATURAL** | Special case for INNER, LEFT OUTER, RIGHT OUTER, FULL OUTER | NATURAL |\n| **USING** | Special case for INNER, LEFT OUTER, LEFT SEMI, RIGHT OUTER, FULL OUTER, LEFT ANTI| USING |\n\n\n## 2. left semi join\n\n当JOIN条件成立时，返回左表中的数据。如果mytable1中某行的id在mytable2的所有id中出现过，则此行保留在结果集中。\n\n```sql\nSELECT * FROM mytable1 a LEFT SEMI JOIN mytable2 b ON a.id=b.id;\n```\n**只会返回mytable1中的数据**，只要mytable1的id在mytable2的id中出现。\n\n## 3. left anti join\n\n当JOIN条件不成立时，返回左表中的数据。如果mytable1中某行的id在mytable2的所有id中没有出现过，则此行保留在结果集中。\n\n```sql\nSELECT * FROM mytable1 a LEFT ANTI JOIN mytable2 b ON a.id=b.id;\n```\n**只会返回mytable1中的数据**，只要mytable1的id在mytable2的id没有出现。\n\n\n## 4. 参考引用\n\n[1] https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-joins.html\n\n[2] https://stackoverflow.com/questions/45990633/what-are-the-various-join-types-in-spark/45990634\n\n[3] https://www.alibabacloud.com/help/zh/doc-detail/73784.htm\n\n[4] https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html\n\n[5] https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html\n\n[6] https://help.aliyun.com/document_detail/73784.html\n\n\n\n\n\n\n\n\n\n","tags":["Join"]},{"title":"Spark SQL - Broadcast Join, Shuffle Hash Join, Sort Merge Join","url":"/2021/02/23/Spark SQL - 3 common joins (Broadcast hash join, Shuffle Hash join, Sort merge join) explained/","content":"\n\n# Spark SQL - Broadcast Join, Shuffle Hash Join, Sort Merge Join\n\n\n## 1. 使用场景\n1. Broadcast Hash Join\n\n    大表和超小表\n\n    spark.sql.autoBroadcastJoinThreshold \n\n    default 10M\n\n2. Shuffle Hash Join\n\n    大表和普通小表\n\n3. Sort Merge Join\n\n    大表和大表\n\n    spark.sql.join.preferSortMergeJoin\n\n\n**COST(Broadcast Hash Join) < COST(Shuffle Hash Join) < COST(Sort Merge Join)**\n\n## 2. hash join\n```sql\nselect a.* from a join b on a.id = b.id;\n```\n1. 小表作为Build Table，大表作为Probe Table。Build Table根据Join Key构建Hash Table，Probe Table根据Join Key进行探测，探测成功则执行join操作。\n2. 构建的Hash Table最好能全部加载在内存，效率最高；内存的限制决定了hash join算法只适合至少一个小表的join场景，对于两个大表的join场景并不适用。\n\n### 2.1 分布式中，HASH JOIN的两个变种\nHash Join适用于大表和小表Join，使用哪种变种需要参考小表有多大\n\n1. Broadcast Hash Join\n超小表使用广播\nSpark SQL也可以根据内存资源、带宽资源适量将参数spark.sql.autoBroadcastJoinThreshold调大，让更多join执行broadcast hash join。\n\n2. Shuffle Hash Join\n一般小表使用shuffle hash join\n\n## 3. Broadcast Hash Join (BHJ)\n\n众所周知，在数据库通用模型（例如星型模型或雪花模型）中，表通常分为两种类型：事实表和维度表。维度表（小型表）通常是指固定的，可变性较小的表，例如联系人，项目等，一般数据是有限的。\n\n事实表通常记录流水，如订单销售明细等，通常随着时间的增长而不断扩大，意味着大表。\n\n当对维度表和事实表进行Join操作时，为了避免shuffle，我们可以将维度表的所有数据分发到每个执行节点供事实表使用。执行程序存储维度表的所有数据，在一定程度上牺牲了空间，替代很费时的shuffle操作，这在SparkSQL中称为Broadcast Join，如下所示：\n\n![bhj](/images/3_common_join_spark_sql/bhj.png)\n   \n## 4. Shuffle Hash Join (SHJ)\n\n![shj](/images/3_common_join_spark_sql/shj.png)\n1.对两个表分别按照join key重新分区，即shuffle，目的是将具有相同join key的记录分配给相应的分区\n\n2.将数据中的对应分区join，这里首先将小表分区构造为hash table，然后根据记录在join key中的大表中的键值取出来进行匹配。\n   \n## 5. Sort Merge Join (SMJ)\n\nspark.sql.join.preferSortMergeJoin\n\nSMJ适用于两张大表的join，两张大表join本身性能很重。\n\n![smj](/images/3_common_join_spark_sql/smj.png)\n\n* shuffle\n\n两张大表都根据key进行重新分区，数据分布到各个节点\n\n* sort\n\n对每个分区节点的两个表的数据分别进行排序\n\n* merge   \n\n对排序的数据进行join操作。遍历两个有序表，碰到相同Join key就merge输出，否则从数据小的一遍继续遍历。\n\n\n## 6. 参考引用\n\n[1] https://sq.163yun.com/blog/article/155507707635548160\n\n[2] https://www.linkedin.com/pulse/spark-sql-3-common-joins-explained-ram-ghadiyaram\n\n[3] https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-SparkStrategy-JoinSelection.html\n\n\n\n\n\n\n\n","tags":["Join"]},{"title":"DorisDB vs ClickHouse","url":"/2021/02/21/Doris vs ClickHouse/","content":"\n\n# DorisDB vs ClickHouse\n\n\n## 1. ClickHouse的缺点\n\n### 1.1 过度依赖大宽表\n* 构建和维护大宽表的工作量令人发指\n* 无法利用星型模型和雪花模型\n\n### 1.2 难以支撑高并发\n* 高并发需要多集群？ 多集群意味着多备份，多运维\n\n### 1.3 运维复杂度\n* 依赖第三方的副本机制\n* zk可能成为瓶颈\n\n\n## 2. Doris建表的最佳实践\n### 2.1 分区和分桶\n![partition](/images/dorisdb_vs_clickhouse/partition.png)\n\nPartition 是数据导⼊和备份恢复的最⼩逻辑单位\n\nTablet 是数据复制和均衡的最⼩物理单位\n\n### 2.2 分区分桶与裁剪\n![table_partition](/images/dorisdb_vs_clickhouse/table_partition.png)\n![partition_tablet](/images/dorisdb_vs_clickhouse/partition_tablet.png)\n\n* 针对 Doris ⽀持两层的数据划分\n\n        1 第⼀层是 Partition 分区，⽀持 Range 的划分⽅式\n\n        2 第⼆层是 Distribution 分桶，⽀持 Hash 的划分⽅式\n\n* Partition 类似分表，是对⼀个表按照分区键进⾏切分，可以利⽤分区裁剪减少数据访问量，也可以根据\n数据的**冷热程度**把数据划分到不同的介质上\n\n* Distribution是对数据在不同BE上的数据分布⽅式，按照分桶键hash以后均匀分布在BE上，注意尽量选择分桶键让数据均衡，不要出现bucket数据倾斜的情况\n\n* Bucket数量的需要适中，如果希望充分发挥性能可以设置为BE数量* CPU core数量 或者 BE数量* CPU core数量 / 2，最好tablet控制在100MB - 1GB之间，tablet太少并⾏度可能不够，太多可能元数据过多，底层scan并发太多性能下降。\n\n### 2.3 排序键\n![short_key](/images/dorisdb_vs_clickhouse/short_key.png)\n\n* 排序键可以使⽤前缀索引（short key）和ZoneMap来进⾏⾼效的过滤，同时也能利⽤数据局部性来加速group by等聚合操作\n\n* shortkey 的列只能是排序键的前缀, 列数不超过3, 字节数不超过36字节, 不包含FLOAT/DOUBLE类型的列,VARCHAR类型列只能出现⼀次, 并且是末尾位置\n\n* 索引粒度1024 类似Clickhouse的index_granularity\n\n* Zonemap 通过min max来快速过滤page中的数据\n\n\n## 3. 整体架构\n![architecture](/images/dorisdb_vs_clickhouse/architecture.png)\n\n### 3.1 MPP架构\n![mpp](/images/dorisdb_vs_clickhouse/mpp.png)\n![mpp2](/images/dorisdb_vs_clickhouse/mpp2.png)\n\n### 3.2 Profile\n![profile](/images/dorisdb_vs_clickhouse/profile.png)\n图形化的Profile 可以看到MPP的执⾏过程，⽅便的定位性能瓶颈和数据倾斜\n\n**这个功能目前开源apache doris没有，只有dorisdb有**\n\n## 4 计算引擎\n### 4.1 向量化技术\n![vectorization1](/images/dorisdb_vs_clickhouse/vectorization1.png)\n![vectorization2](/images/dorisdb_vs_clickhouse/vectorization2.png)\n\n* 向量化技术\n内存结构按照列组织 引⼊Column结构，按列load数据，按列进⾏表达式计算和节点计算\n\n* 优势\nCPU 预取 / 分⽀预测友好 / Cache 友好 / ⽅便使⽤SIMD指令集\n\n### 4.2 SIMD 指令\n![SIMD](/images/dorisdb_vs_clickhouse/SIMD.png)\n\n### 4.3 内存优化\n![main_memory](/images/dorisdb_vs_clickhouse/main_memory.png)\n\n### 4.4 Join优化\n![join1](/images/dorisdb_vs_clickhouse/join1.png)\n![join2](/images/dorisdb_vs_clickhouse/join2.png)\n\n### 4.5 聚合优化\n⾃适应两层聚合\n![agg](/images/dorisdb_vs_clickhouse/agg.png)\n\n* 分布式两阶段聚合\n\n* Local Agg ：本地部分聚合\n\n* Final Agg ： 全量聚合\n\n* Steamming Vs TwoLevel Vs Suflfle By Column\n\n* 运⾏时⾃适应\n\n## 5 存储引擎\n\n### 5.1 列式存储\n\n![column](/images/dorisdb_vs_clickhouse/column.png)\n![column2](/images/dorisdb_vs_clickhouse/column2.png)\n\n### 5.2 前缀索引\n![short](/images/dorisdb_vs_clickhouse/short.png)\n\n### 5.3 Bitmap索引\n![bitmap](/images/dorisdb_vs_clickhouse/bitmap.png)\n\n### 5.4 延迟物化\n![late_materialization](/images/dorisdb_vs_clickhouse/late_materialization.png)\n\n### 5.5 向量化过滤\n![vectorization3](/images/dorisdb_vs_clickhouse/vectorization3.png)\n\n### 5.6 低基数优化字典编码\n![low_cardinality](/images/dorisdb_vs_clickhouse/low_cardinality.png)\n\n## 6 极速MPP\n![the_mpp](/images/dorisdb_vs_clickhouse/the_mpp.png)\n\n## 7 除了快，还有什么\n### 7.1 使用简单\n* Mysql协议兼容\n    * 标准SQL，兼容各类主流BI，迁移成本极低\n    * 函数⽀持丰富，包括各类时间、字符串、GIS、聚合函数以及窗⼝函数\n* 灵活的数据模型\n    * ⾼效⽀持星型/雪花模型，避免打平成⼤宽表，更加灵活的适应实时更新和维度变化\n    * Duplicate/Unique/Aggregate三种模式适配各类场景\n* 开箱即⽤的⽤户体验\n    * ⾃适应应对各种case，⽆须额外的复杂优化⼿段\n    * 各种Load⼿段可以降低ETL⼯作量，通过配置快速导⼊数据\n    \n### 7.2 运维方便\n* ⾼可⽤\n    * FE采取类Paxos⼀致性算法，⽆单点瓶颈\n    * BE多副本可以独⽴调节，数据按⾃身特性灵活拆分\n* ⾼度⾃治\n    * ⽆外部依赖，避免繁杂的中间件维护\n    * 副本⾃修复，数据⾃均衡，⼀键扩容，缩容\n* 可视化管理⼯具\n    * 全局配置，⼀键部署，滚动升级\n    * 完备的监控告警集成保障系统稳定可观测\n    * 灵活的扩容缩容⽅便集群容量规划\n\n### 7.3 生态完善\n![sys](/images/dorisdb_vs_clickhouse/sys.png)\n\n### 7.4 稳定可靠\n* 完善的测试\n    * 完成SSB TPCH TPCDS NYC-TAXI 等测试集功能完备\n    * SQLancer 每天构建数千万条随机SQL验证\n    * 引⼊SQLite/Spark等外部DB 200万标准测试集通过\n* ⾼可⽤架构\n    * 没有单点，FE ⾼可⽤，BE多副本\n    \n### 7.5 统一分析平台\n![general](/images/dorisdb_vs_clickhouse/general.png)\n\n## 8 DorisDB Vs. ClickHouse\n\n|  | ClickHouse | DorisDB |\n| ----- | ---- | ---- |\n| **标准SQL语⾔的⽀持** | 不⽀持标准的SQL语⾔，⽆法直接对接主流的BI系统。 |⽀持标准的SQL语⾔，兼容MySQL协议，可以直接对接主流的BI系统。 |   \n| **分布式Join** | ⼏乎不⽀持分布式Join，在分析模型上仅⽀持⼤宽表模式。 | ⽀持各种主流分布式Join，不仅⽀持⼤宽表模型，还⽀持星型模型和雪花模型。|\n| **⾼并发查询** | 传统MMP数据分布⽅式，⼩查询会极⼤消耗集群的资源，⽆法实现⾼并发查询，并且⽆法通过扩容的⽅式来提⾼并发能⼒。 | 现代化MPP数据分布⽅式，数据按照分⽚的⽅式保存，⼩查询只需要⽤到部分机器资源，能极⼤地提⾼并发查询量。 | \n| **MPP架构** | Scatter-Gather 模式，聚合操作依赖单点完成，操作数据量⼤时存在明显的性能瓶颈。 | 现代化MPP架构，可以实现多层聚合，能够执⾏复杂的SQL查询，⼤表Join，⾼基数聚合查询等。 |  \n| **精确去重** | 对⾼基数列进⾏精确去重操作（如计算APP的DAU）时，受限于单点聚合的处理⽅式，性能瓶颈明显。 | 现代化MPP架构，可以实现多层聚合，能有效利⽤多机资源，保证查询性能。\n| **Exactly once语义** | 数据导⼊⽆事务保证，⽆法保证数据写⼊的“不丢不重”，订单类场景⽆法使⽤。 | 数据导⼊有事务保证，可以很容易地实现Exactly once语义，数据导⼊“不丢不重”。 \n| **集群扩容** | 传统MPP数据分布⽅式，数据扩容时需要进⾏数据重分布，需要⼈⼯操作，⼯作量巨⼤，影响线上服务。 | 现代化MPP数据分布⽅式，扩容时只需要迁移部分数据分⽚⾛即可，系统⾃动完成，不影响线上服务。\n| **运维** | 分布式⽅案依赖Zookeeper，在集群扩⼤时，Zookeeper会变成性能瓶颈，额外运维和维护成本⾼。 | 不依赖任何外部系统，整个系统只有两种进程，⾃动故障恢复，极简运维。 |\n| **社区⽣态** | 整个开源社区被俄罗斯公司把持，在中国没有商业化公司⽀持，使⽤上规模后技术⽀持⽆法保证。 | 开源社区的核⼼研发⼈员都是中国⼈，在国内有商业化公司⽀持，服务更加本地化，技术⽀持⽆障碍。 |\n\n## 9 参考引用\n[1] https://zhuanlan.zhihu.com/p/344031483\n\n[2] DorisDB 技术分享会PPT\n\n\n\n\n\n\n\n\n","tags":["OLAP"]},{"title":"浅谈分布式数据库技术","url":"/2021/02/07/数据库技术基础/","content":"\n# 浅谈分布式数据库技术 (计算，优化，存储)\n\n## 编译执行和解释执行\n* 解释执行：火山模型和向量化执行\n\n常用于OLTP,  也用于一些老的OLAP\n\nOracle, MySQL, SQL Server, Doris\n\n* 编译执行：\n通常用语OLAP分析\nHyPer, MemSQL, Hekato, Impala, Spark Tungsten[2]\n\n## 火山模型(Volcano Iterator Model)\n火山模型[1] \n\n因为在90年代，计算机的内存资源昂贵，相比于CPU的执行效率，IO效率要更差(IO墙效应)，因此火山模型将内存资源更多地用于IO的缓存设计，没有优化CPU的执行效率[3]\n\n关系代数算子(relational-algebraic operator/RA operator)：\n\ntuple stream:\n\ninterface: open, next, close; next调用从stream中产生新的tuple\n\n虚函数调用开销：next调用导致很深的虚函数嵌套，编译器无法对虚函数进行inline优化，每次虚函数调用都需要查找虚函数表。\n\nCPU分支预测不友好：复杂的虚函数嵌套容易导致预测失败并且流水线变得混乱->CPU执行效率低下\n\ninstruction：指令\n\nscalar:标量 a tuple data\n\nvector:矢量 a vector(batch) of data\n\nloop unrolling or loop pipelining: 循环展开 循环流水线\n\n缺点：\n指令缓存未命中 instruction cache misses：\n指令和数据会被加载到Cache Memory中，如果查询过大，生成的execution code会更多，指令也会更多，并且一个general query execution code会导致更多的branching，这些指令都缓存到Cache Memory，那缓存中会存在更多的无用的指令。\n更容易导致缓存未命中。\n因为分支(branching)和抽象(abstraction)的问题，为了实现一个通用的query execution code generator，肯定会存在很多分支和抽象。\n\n## 解决火山模型导致 instruction cache misses 问题的两种方式：向量化执行和编译执行\n向量化执行缓解了指令缓存未命中的问题。\n编译执行解决了指令缓存未命中的问题。\n\n### 向量化执行(Vectorized Query)\n向量化的是instruction还是data?\ndata\n\n利用DLP(Data Level Parallelism)，使用单指令操作多条记录\n单指令是指无需为每一行记录频繁地load指令，加载一次指令，并操作一批数据。这样就可以减少instruction cache misses\n\n向量化执行：火山模型的改进方案，火山模型是tuple-at-a-time的实现，向量化执行是batch-tuple-at-a-time。\n\n每个算子执行完传递一行给上游算子继续执行，函数调用过多，且大量的虚函数调用，条件分支预测失败->CPU利用率低。\n\n缓解了instruction cache misses问题，但没有根治。\n\n### 编译执行(Compilation Query)\n* 什么是编译执行？\n从编译层面解决instruction cache misses的问题。\n\nexecution code generation 不应该包含不必要的部分：包括branching和abstraction。\n\nVolcano Iterator Model是非常通用的，适合所有查询，所以包含了很多branching和abstraction。\n\n每次针对查询本身生成特定代码，消除冗余branching和abstraction。\n\n* 编译执行如何实现？\n* 确定各种算子和算子的执行顺序\n* 合并各种函数调用为一个\n* 基于存储格式和硬件支持，应用优化，比如循环展开，循环流水线和SIMD指令\n\n优点：\n* 没有虚函数调用\n* 数据在寄存器中\n* 编译器循环展开，循环流水线，SIMD的应用\n\n缺点：\n* 编译开销：为查询生成定制的查询代码存在一定开销。对于数据量很少的查询来说，编译开销可能比执行开销更小。\n\n\n## SIMD\nsingle instruction multiple data\n\n## LLVM JIT\n## CodeGen\n## Operator 算子\n* project\n* filter\n* scan\n* join\n* Aggregate\n* union\n\n## 词法分析 语法分析 语义分析 AST 关系代数表达式(RA) \n编译器分为frontend和backend，lexer是frontend的第一阶段。\n### 词法分析(lexer/scanner)\n如：jflex \n词法单元分类：\nidentifier: names the programmer chooses;\nkeyword: names already in the programming language;\nseparator (also known as punctuators): punctuation characters and paired-delimiters;\noperator: symbols that operate on arguments and produce results;\nliteral: numeric, logical, textual, reference literals;\ncomment: line, block (Depends on the compiler if compiler implements comments as tokens otherwise it will be stripped).\n\nExamples of token values\nToken name\t        Sample token values\nidentifier\t            x, color, UP\nkeyword\t                if, while, return\nseparator\t            }, (, ;\noperator\t            +, <, =\nliteral\t                true, 6.02e23, \"music\"\ncomment\t                /* Retrieves user data */, // must be negative\n\n输入：\nx = a + b * 2;\n输出：(token name, token value)\n[(identifier, x), (operator, =), (identifier, a), (operator, +), (identifier, b), (operator, *), (literal, 2), (separator, ;)]\n\n### 语法分析(parser/syntax analysis)\nscannerless parser：将lexer和parser合并为一步，并发执行。\nparser是frontend的第二阶段。\n如：cup\n\n实现：\n* 正则表达式\n* 模式匹配\n\ntoken生成表达式Expr，在context-free语法下，按照token出现的*顺序*，递归组成Expr。\n\n输入：tokens\n\n输出: parse tree / abstract syntax tree 抽象语法结构树\n### 语义分析 semantic analysis/ context-sensitive analysis\n验证表达式含义，并执行适当的操作: 从source code收集语义信息，类型检查，变量声明。\n\n编译器\n\n输入：AST?\n\n输出：\n### AST Abstract Syntax Tree\nAST: source code的抽象语法结构\n抽象：没有展示source code中出现的所有细节\n\n\n输入：\n\n输出： \n### 由 AST 到关系代数表达式，可以使用 visitor 模式遍历\n### 查询优化器以关系代数表达式作为输入，输出查询执行计划。逻辑执行计划/物理执行计划。 单节点执行计划/分布式执行计划\n\n## 虚函数 \n虚函数：定义了要执行的目标函数，但目标的具体指向在编译时无法确定。\n\n## spark tungsten\n\n\n特性：\n* 可以被继承和覆盖，在面向对象中是多态的重要组成部分\n* Java中所有非final类型的方法都是虚函数\n\n## PIPELINE\n上游stage不需要等下游stage完全执行结束就可以拉取数据并执行计算\n\n## 存储系统 数据模型 数据组织 数据格式\n## 列存储 ORC PARQUET\n##  Cumulative Compaction / Base Compaction\n## RBO CBO\n## MPP SCATTER/GATHER\n## SQL Expr\n* literal\n* expr\n* predicate\n* conjunction \n* function\n* ref\n* slot\nPREDICATE CONJUNCTION\n\nFROM \nJOIN ON  \nWHERE \nORDER BY \nGROUP BY \nHAVING\n\n\n## MVCC\n## 线性一致性/强一致性\n## \n\n[1] Vectorized and Compiled Queries — Part 1 https://medium.com/@tilakpatidar/vectorized-and-compiled-queries-part-1-37794c3860cc\n[2] 数据库查询引擎的进化之路 https://zhuanlan.zhihu.com/p/41562506\n[3] Volcano, an Extensible and Parallel Query Evaluation System\n\nSQL原理解析\nhttps://mp.weixin.qq.com/s/v1jI1MxEHPT5czCWd0kRxw","tags":["SQL"]},{"title":"Spark Tungsten：优化Spark核心执行引擎","url":"/2021/02/07/Spark-Tungsten/","content":"\n\n# Spark Tungsten(钨)\n\n## 1. 什么是Tungsten\n\nTungsten是自Spark项目创建以来对Spark执行引擎的最大修改，从本质上改善了Spark应用的内存和CPU执行效率，将性能向到现代硬件的极限推进。\n\n事实上，Spark工作负载越来越受到CPU和内存的限制，而不是磁盘IO和网络通信，引发了大家对CPU执行效率的关注，最近的大数据工作负载性能研究表明了这一个趋势。\n\n### 1.1 为什么CPU成为了瓶颈\n* 硬件角度：现在的硬件配置提供了越来越大的总IO带宽，比如10Gbps的网络连接, 高带宽的SSD或者条带HDD阵列用于存储。\n* 软件角度：spark optimizer现在通过修剪不必要的输入数据，可以避免大量磁盘IO的工作。在spark shuffle子系统中，序列化和hashing(CPU bound) 成了关键瓶颈，而不是底层硬件的网络吞吐量。\n\n\n### 1.2 Tungsten包含了哪些优化\n* 内存管理和二进制处理：利用应用程序语义(sun.misc.Unsafe)显式管理内存，消除JVM对象模型和GC的开销\n* Cache-aware computation(缓存感知计算)： 利用memory hierarchy（内存阶层）的算法和数据结构\n* Code Generation: 利用现代编译器和CPU生成代码\n* 没有Virtual Function Dispatches： 减少复杂的CPU调用，虚函数调度多时，会对性能有影响\n* 中间数据在内存 VS. 中间数据在CPU寄存器：Tungsten Phase 2将中间数据放到CPU寄存器，相比于从内存获取数据，从CPU寄存器获取数据的周期数减少了一个数量级。\n* loop unrolling(循环展开)and SIMD: 更好地利用现代编译器和CPU能力进行编译，执行简单的循环，而不是复杂的函数调用。\n\n\n## 2. 内存管理和二进制处理（binary processing）\n\nJVM应用依赖垃圾收集器来管理内存，随着spark应用性能推到极限，JVM对象开销不可忽视。\n### 2.1 Java对象占据了太多内存\nJVM对象占据了大量的固有（inherent）内存开销。比如说，字符串\"abcd\"使用UTF-8编码时占用4个bytes。JVM本地的字符串实现，以不同方式存储以适应更多场景。每个character编码\n使用UTF-16耗费2bytes， 每个String对象包含12bytes的header和8bytes的hashcode。\n```\njava.lang.String object internals:\n\nOFFSET  SIZE   TYPE DESCRIPTION                    VALUE\n     0     4        (object header)                ...\n     4     4        (object header)                ...\n     8     4        (object header)                ...\n    12     4 char[] String.value                   []\n    16     4    int String.hash                    0\n    20     4    int String.hash32                  0\nInstance size: 24 bytes (reported by Instrumentation API)\n```\n**一个简单的4bytes字符串在JVM对象模型中占用了超过48bytes！！！**\n\n### 2.2 Java垃圾回收效率可能会很差\n\nJVM对象模型的另一个重要开销是内存回收。年轻代需要高频地分配和回收内存，当GC可以可靠地估算对象的生命周期时会很有效，但是当GC不能有效估算对象的生命周期时，效果会很差。\n因此如果想要获得性能，需要使用GC调优的黑魔法，使用大量关于对象生命周期的JVM参数，提供更多关于JVM的信息。\n\n因为Spark应用不仅仅是通用程序，Spark了解数据如何在计算的各个阶段以及job和task的范围内流动。Spark比JVM垃圾收集器更了解内存块的生命周期，\n可以比JVM更有效地管理内存。\n\n### 2.3 通过显式管理内存和二进制数据（而不是java对象模型）来解决\n为了解决对象开销和GC效率低下的问题，Spark引入了显式的内存管理器，将大部分的Spark操作转换为直接操作二进制数据，而不是java对象。\n这是基于JVM提供的```sun.misc.Unsafe```实现的高级功能，暴露了c-style的内存访问（显示分配，释放，指针计算）。\n另外，Unsafe方法是固有(intrinsic)的，意味着每个方法调用都由JIT编译为单独的一条机器指令（machine instruction）。\n\nSpark 1.4 引入了新的hash map对DataFrames和SQL聚合，1.5会有更多的数据结构来满足大部分操作，包括sorting和joins。\n\n![new-hash-map](/images/tungsten/new-hash-map.png)\n总的来说，通过显式管理内存的手段，直接操作二进制数据, 而不是java对象, 可以消除GC开销, 提升Spark性能。\n\n## 3. Cache-aware Computation\n\n### 3.1 什么是in-memory Computation\n\nSpark可以高效利用集群内存资源，比基于磁盘处理的解决方案高效很多。Spark也可以处理远大于内存的数据量，将数据透明地spill到磁盘，执行外部操作，比如排序和hash。\n\n### 3.2 什么是cache-aware Computation\n\n相似地，cache-aware computation 通过对L1/L2/L3级别的CPU cache的高效利用，可以更快地处理数据，比利用主存(main memory)还要快好几个数量级。\n\n在对Spark应用进行性能分析时，发现大部分的CPU时间都是在等待从main memory中fetch数据。因此设计了cache-friendly的算法和数据结构使得Spark\n应用在提取数据上花费更少的时间。\n\n\n### 3.3 举个栗子\n记录（records）排序：标准排序过程是保存一个records的指针**数组**，使用快排来交换指针，直到所有records有序。因为顺序扫描访问，\n所以排序很容易有好的cache hit rate，但是对指针**列表**排序，会有更低的cache hit rate，因为对比操作需要对两个(指向内存中任意位置的记录的)指针取消引用。\n![cache-aware](/images/tungsten/cache-aware.png)\n\n如何改善排序的缓存本地性？简单的方式是和指针一起，保存每个记录的key。如果sort key是64-bit integer, 就是用128-bit\n(64-bit pointer和64-bit key)将每个记录保存在指针数组中。这样快排就可以使用pinter-key pairs，线性排序，而不需要随机内存访问。\n\n在Spark中，大部分操作都是可以简化为a small list of operations，比如aggregations，sorting和join。通过改善这些operation的效率。\n可以整体改善Spark应用的效率。\n\n## 4. 代码生成\nSpark 之前在SQL和DataFrames中引入了code generation for expression evaluation,  解析表达式（Expression Evaluation）是对于特定\n记录如（```age = 37 ```）计算一个表达式如（``` age < 40 and age > 35 ```）的过程。\n\nSpark在运行时动态生成字节码，用于解析这些表达式。而不是通过较慢的解释器逐步解析表达式。\n\n相比于解释执行，代码生成：\n1 减少了基础类型的装箱\n2 更重要的是，避免昂贵的多态函数调度(虚函数调用)\n\n### 4.1 vectorized expression\n当前Spark已经把代码生成扩展应用到了大部分内置表达式，并且计划把代码生成的级别从record-at-a-time提升到vectorized expression, 通过JIT\n的能力，利用现代CPU中更好的指令流水线，实现一次处理多条记录。\n\n### 4.2 除了expression evaluation的代码生成, 还有其他领域的代码生成应用\n1 内存二进制格式到wire-protocol的转换，用于shuffle。shuffle 当前经常以数据序列化为瓶颈，而不是下层的网络。通过代码生成，可以增加序列化\n的吞吐量，相应的增加网路吞吐量。\n![code-gen](/images/tungsten/codegen.png)\n单线程，八百万复杂数据记录的性能 -> (Kryo vs. CodeGen) CodeGen的性能是Kryo性能的两倍\n\n\n## 5 Tungsten和Tungsten之外\n\n### 5.1 Spark应用Tungsten\n\nTungsten项目是Spark核心引擎设计的巨大变动。\n\nSpark 1.4 在DataFrame API中实现了aggregation operations的内存显式管理，同时支持自定义序列化器。\n\nSpark 1.5 扩展了二进制内存管理和cache-aware数据结构的覆盖范围。\n\n### 5.2 Tungsten之外的待研究和优化点\n\n研究对LLVM或者OpenCL的编译，因此Spark应用程序可以利用现代CPU中的SSE/SIMD指令，以及GPU中广泛的并行性来加快机器学习和图形计算的操作。\n\n\n## 参考引用\n\n[1] https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\n\n[2] https://databricks.com/glossary/tungsten\n\n[3] https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n\n[4] https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html\n\n[5] https://www.linkedin.com/pulse/catalyst-tungsten-apache-sparks-speeding-engine-deepak-rajak/?articleId=6674601890514378752\n\n","tags":["Spark"]},{"title":"分布式共识算法Basic Paxos","url":"/2021/02/02/分布式共识算法-Basic-Paxos/","content":"\n\n\n# Basic Paxos\n## 什么是Paxos\n* Paxos是解决分布式共识问题的一系列算法，由 [Leslie Lamport](https://lamport.azurewebsites.net/) 提出，大神的几篇关于Paxos的论文：\n\n    [1998-The Part-Time Parliament](https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)\n\n    [2001-Paxos Made Simple](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)\n\n    [2004-Cheap Paxos](http://lamport.azurewebsites.net/pubs/web-dsn-submission.pdf)\n\n    [2005-Fast Paxos](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2005-112.pdf)\n\n* 共识(Consensus)是多个节点就某个提案(n, v)达成统一结果的过程。因为参与者本身会失败，或者参与者之间的通信会失败，比如网络分区，消息延迟，节点故障，所以这是个棘手的问题。\n\n## Basic Paxos\n* 就单个值达成共识，如果大多数节点就单个值达成共识，值就不会再变了。不涉及连续多个值的共识协商。\n\n### 角色\n* 客户端(Client)：向分布式系统发送请求，并接收回复。\n* 提议者(Proposer)：接受客户端的一个请求并支持该请求，之后尝试让其他接受者支持相同的请求。\n    接入和协调功能，收到客户端请求后，发起 *二阶段提交* ，进行共识协商\n* 接受者(Acceptor)：对提案进行投票，存储接受的提案。\n* 学习者(Learner)：不参与投票，在接受者达成共识之后，存储保存。\n一个节点可以同时扮演多个角色，不影响算法正确性，一个节点扮演多个角色可以降低延迟，减少节点间消息发送\n\n### 提案\n* (n, v) n为提案编号，v为提案值\n* 提案编号特性：原子，递增，持久存储\n\n### Quorum\n本义指的是出席议会议事的最低议员数量。\n在Paxos中，接受者组成Quorums。发送给某一个接受者的消息必须发送给Quorum中的所有其他接受者。\n\n## 二阶段提交\n* Phase 1：准备阶段\n\n    a. Prepare: 创建一个提案(n, v)，发起二阶段提交, n必须是递增的。将Prepare message(只包含提案编号n，不需要包含v)发给a quorum of acceptors\n\n    b. Promise: 接受者比较n和自己已经响应的提案的最大提案编号：\n    \n        1. 如果小于等于已经响应的最大提案编号，则忽略，可以不响应，也可以响应以防止提议者反复提交提案编号为n的Prepare message。\n    \n        2. 如果大于已经响应的最大提案编号，接受者返回给提议者Promise，并忽略所有提案编号小于n的提案；如果接受者之前接受了某个提案(m, w)，则接受者必须将这个提案信息返回给提议者。\n* Phase 2：接受阶段\n\n    a. Accept: \n    \n        1. 如果接受者已经达成共识(m, w)，则Accept message为(n, w)\n        \n        2. 如果提议者接收到a quorum of acceptors中大多数节点的Promises，会设置节点值为提案值v，提议者发送Accept message(n, v)\n    \n    b. Accepted: \n    \n        1. 接受者之前尚未达成共识，Accept message(n, v)\n        \n        2. 接受者之前已达成共识(m, w), Accept message(n, w)\n\n\n### Basic Paxos几个原则\n* 如果准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，则接受者不会响应这个准备请求\n\n* 如果接受请求的提案编号，小于接受者已经响应的准备请求的提案编号，则接受者不会通过这个提案\n\n* 如果接受者已经通过提案，那么接受者会在准备请求的响应中，包含已通过的最大编号的提案信息\n\n* 如果大多数接受者对某个提案达成共识，那么提案值就不会再变了，而提案编号会因为少数节点(尚未通过任何提案)接收到新的准备请求而使用递增的更大的提案编号。\n\n\n### 二阶段提交场景 \n一个Quorum: A,B,C\n* 场景1：节点A通过了准备请求(1,v1)，但还没有接受提案(1,v1)，如果此时节点A接受到新的准备请求(2, v2)，因为新的准备请求的提案编号2大于之前的提案编号1，则节点A保证不再响提案编号小于等于2的准备请求，也不会通过提案编号小于等于2的提案。\n\n* 场景2：节点A,B已经通过了提案(2, v1),节点C未通过提案，此时客户端发起请求设置(3, v2), 因为AB已经通过提案(大多数节点)，所以提案值不会变动，而提案编号使用最大的3，则最终节点值(3, v1)\n\n\n## 领导者选举\n* 可以通过Basic Paxos算法实现领导者选举，也就是多个节点就提案达成共识的过程。在Chubby中，领导者选举就是通过Basic Paxos实现的。\n","tags":["Doris"]},{"title":"Doris Aggregate Model踩坑","url":"/2021/02/01/Doris-Aggregate-Model/","content":"\n\n# Doris Aggregate Model踩坑\n## 聚合模型的特性\n\n* Doris不允许修改 Value 列的聚合类型(REPLACE/MIN/MAX/SUM)。\n\n* Doris聚合模型添加列默认为Key，如果添加 Value, 需要指定聚合类型。\n\n\n## 踩坑\n**Doris Aggregate Model默认添加列类型为Key**\n\n线上有一张表是 Aggregate Model, Value 类型包含了 SUM 和 REPLACE, 现在根据业务需求添加 Value 字段。\n执行\n\n```\nalter table tmp add column col1 bigint;\n```\n\n因为 Aggregate Model 新建列默认是 Key 列，以上sql执行结束后col1列出现在了 Key 列，而不是期望的 Value 列。\n为了解决这个问题，做了以下尝试：\n\n1 是否可以 drop Key 列？\n```\nalter table tmp drop column col1;\n```\n执行失败：drop Key列时表中不允许存在REPLACE类型的列。\n\n2 因为REPLACE类型的列在tmp表中无关紧要，是否可以drop REPLACE类型的 Value 后，drop Key列\n```\nalter table tmp drop column value_replace; // 执行成功\nalter table tmp drop column col1; // 执行成功\n```\n\n3 重新建表刷数据\n这个方法最直接，也最重。当然可以实现。\n\n正确的添加 Value 列的方法是\n```\nalter table tmp add column col1 bigint sum comment \"字段\";\n```\n\n\n## help alter table\n执行help alter table后，关于添加列的说明：\n\n```\n    1. 向 example_rollup_index 的 col1 后添加一个Key列 new_col(非聚合模型)\n        ALTER TABLE example_db.my_table\n        ADD COLUMN new_col INT Key DEFAULT \"0\" AFTER col1\n        TO example_rollup_index;\n\n    2. 向example_rollup_index的col1后添加一个value列new_col(非聚合模型)\n          ALTER TABLE example_db.my_table\n          ADD COLUMN new_col INT DEFAULT \"0\" AFTER col1\n          TO example_rollup_index;\n\n    3. 向example_rollup_index的col1后添加一个Key列new_col(聚合模型)\n          ALTER TABLE example_db.my_table\n          ADD COLUMN new_col INT DEFAULT \"0\" AFTER col1\n          TO example_rollup_index;\n\n    4. 向example_rollup_index的col1后添加一个value列new_col SUM聚合类型(聚合模型)\n          ALTER TABLE example_db.my_table\n          ADD COLUMN new_col INT SUM DEFAULT \"0\" AFTER col1\n          TO example_rollup_index;\n\n    5. 向 example_rollup_index 添加多列(聚合模型)\n        ALTER TABLE example_db.my_table\n        ADD COLUMN (col1 INT DEFAULT \"1\", col2 FLOAT SUM DEFAULT \"2.3\")\n        TO example_rollup_index;\n```","tags":["Doris"]},{"title":"Yarn Application开发","url":"/2020/11/29/Yarn Application开发/","content":"\n\n# Yarn Application开发\n\n## 1. Yarn架构\n\n![yarn](/images/yarn_application/yarn.png)\n\nYARN的本质在于将资源管理和作业调度监控的功能分离，在编写Application之前，需要熟悉YARN中的几个重要角色：RM，NM，AM等。\n\n\n\n### 1.1 ResourceManager(RM)\n#### 1. Scheduler\n\n\"纯\"资源Scheduler，只负责调度和分配资源，不会监控Application状态，包括FairScheduler和CapacityScheduler等。\n\n#### 2. ApplicationsManager(ASM)\n\n负责接收管理整个系统中所有应用程序的提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。RM只负责监控AM，在AM运行失败时候启动它，RM并不负责AM内部任务的容错，这由AM来完成。\n\n\n### 1.2 NodeManager(NM)\n运行容器，监控容器资源(CPU，内存，磁盘，网络)使用情况，并报告给RM。\n\n接收并处理来自AM的Container启动/停止等各种请求。\n\n### 1.3 ApplicationMaster（AM）\n\n每个Application都有一个ApplicationMaster，负责向Scheduler申请运行Application需要的Container，协调Application运行，监控Application执行状态和进度，ApplicationMaster负责将一个应用分割成多个任务。RM只负责监控AM，在AM运行失败时候启动它，RM并不负责AM内部任务的容错，这由AM来完成。\n\n### 1.4 Container\n\nYARN中的资源抽象，封装了NM的资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM返回的资源的用Container表示，YARN会为每个任务分配一个Container。现在YARN仅支持CPU和内存两种资源，使用轻量级资源隔离机制Cgroups进行资源隔离。\n\n\n## 2. YARN应用开发流程\n### 2.1 YARN Client\n\n示例代码：\n\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java\n\n**Client<-->ResourceManager**\n\n**Interface YarnClient**\n\n1. 初始化并启动Yarn Client\n\n2. 创建application，并获取application id\n\n3. 设置application的上下文-ApplicationSubmissionContext\n\n```\n    1. Application info: id， name\n\n    2. Queue, priority info：application提交队列， application分配的priority\n\n    3. User：提交application的用户\n\n    4. ContainerLaunchContext\n\n        4.1 R: local Resources (binaries, jars, files etc.)\n\n        4.2 E: Environment settings (CLASSPATH etc.)\n\n        4.3 C: the Command to be executed\n\n        4.4 T: security Tokens (RECT)\n```\n\n4. 提交application：submitApplication\n\n5. 追踪进度：getApplicationReport\n\n6. 杀掉application：killApplication\n\n\n### 2.2 Application Master\n\n示例代码：\n\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java\n\n**ApplicationMaster<-->ResourceManager**\n\n**Interface AMRMClientAsync**\n\n1. AM启动后，分别启动AMRMClient和NMClient\n\n2. AM通过AMRMClient向RM注册自己，和RM保持心跳：registerApplicationMaster\n\n3. 申请Container：addContainerRequest\n\n4. 运行Containers并通过心跳将进度报告给RM\n\n5. ApplicationMaster确认作业结束后，通过AMRMClient从RM取消注册：unregisterApplicationMaster\n\n\n\n**ApplicationMaster<-->NodeManager**\n\n**Interface NMClientAsync**\n\n1. 和启动AM一样，初始化ContainerLaunchContext的RECT\n\n2. 通过NMClientAsync启动执行Task的Container：startContainerAsync\n\n3. NMClientAsync处理Container启动，停止，状态更新\n\n\n## 示例\n\n```bash\nhadoop jar \\\n/opt/cloudera/parcels/CDH/jars/hadoop-yarn-applications-distributedshell-2.6.0-cdh5.16.1.jar \\\n org.apache.hadoop.yarn.applications.distributedshell.Client \\\n   --jar /opt/cloudera/parcels/CDH/jars/hadoop-yarn-applications-distributedshell-2.6.0-cdh5.16.1.jar \\\n   --shell_command ls \\\n   --num_containers 10 \\\n   --container_memory 350 \\\n   --master_memory 350 \\\n   --priority 10\n```\n\n\n\n\n\n\n\n\n","tags":["YARN"]},{"title":"调度系统常见问题","url":"/2020/11/02/调度系统常见问题/","content":"\n\n# 调度系统常见问题\n\n任务调度系统作为大数据平台的核心组件，涉及的场景繁多，各个公司的数据平台实现的方案也有所不同，自研或者在开源系统基础上做二次开发，都会有适合自己的调度实现方案。\n\n现在的开源调度也有很多，比如airbnb的airflow，易观apache olphinscheduler，azkaban，阿里开源的zeus等等。\n\n![scheduler](/images/scheduler/scheduler.png)\n\n\n\n\n\n\n\n\n\n\n","tags":["大数据任务调度"]},{"title":"Hive Spark Impala 高频参数配置","url":"/2020/10/24/Hive Spark 高频配置参数/","content":"\n# Hive Spark Impala 高频参数配置\n\n## Hive\n\n\n[https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties](https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties)\n\n| 参数 |  |\n| ----- |  |\n| **hive.exec.dynamic.partition** | |\n| **hive.exec.dynamic.partition.mode** | |\n| **hive.exec.max.dynamic.partitions** | |\n| **hive.exec.max.dynamic.partitions.pernode** |  |\n| **hive.execution.engine** | |\n| **hive.exec.max.created.files** |  |\n| **hive.exec.parallel** |  |\n| **hive.mapred.mode** |  |\n| **hive.merge.mapredfiles** |  |\n| **hive.optimize.sort.dynamic.partition** |  |\n| **hive.auto.convert.join** | |\n| **hive.auto.convert.join.noconditionaltask** | |\n| **hive.compute.query.USING.stats** | |\n| **hive.exec.dynamic.partition** | |\n| **hive.exec.dynamic.partition.mode** | |\n| **hive.exec.max.created.files** | |\n| **hive.exec.max.dynamic.partitions** | |\n| **hive.exec.parallel.thread.number** | |\n| **hive.exec.reducers.bytes.per.reducer** | |\n| **hive.fetch.task.conversion** | |\n| **hive.fetch.task.conversion.threshold** | |\n| **hive.groupby.skewindata** | |\n| **hive.input.format** | |\n| **hive.limit.pushdown.memory.usage** | |\n| **hive.map.aggr** | |\n| **hive.map.aggr.hash.percentmemory** | |\n| **hive.merge.mapfiles** | |\n| **hive.merge.mapredfiles** | |\n| **hive.merge.size.per.task** | |\n| **hive.merge.smallfiles.avgsize** | |\n| **hive.merge.sparkfiles** | |\n| **hive.optimize.bucketmapjoin.sortedmerge** | |\n| **hive.optimize.index.filter** | |\n| **hive.optimize.ppd** | |\n| **hive.optimize.reducededuplication** | |\n| **hive.optimize.reducededuplication.min.reducer** | |\n| **hive.optimize.skewjoin** | |\n| **hive.smbjoin.cache.rows** | |\n| **hive.stats.autogather** | |\n| **hive.stats.fetch.column.stats** | |\n\n## Spark\n[https://spark.apache.org/docs/latest/configuration.html](https://spark.apache.org/docs/latest/configuration.html)\n\n| 参数 |  |\n| ----- |  |\n| **spark.master** | |\n| **spark.serializer** | |\n| **spark.sql.autoBroadcastJoinThreshold** | |\n| **spark.default.parallelism** | | \n| **spark.executor.memory** | | \n| **spark.sql.adaptive.enabled** | | \n| **spark.sql.adaptive.shuffle.targetPostShuffleInputSize** | | \n| **spark.sql.crossJoin.enabled** | | \n| **spark.sql.shuffle.partitions** | | \n| **spark.driver.memory** | |\n| **spark.dynamicAllocation.enabled** | |\n| **spark.dynamicAllocation.maxExecutors** | |\n| **spark.dynamicAllocation.minExecutors** | |\n| **spark.executor.cores** | |\n| **spark.executor.memoryOverhead** | |\n| **spark.kryoserializer.buffer** | |\n| **spark.kryoserializer.buffer.max** | |\n| **spark.sql.shuffle.partitions** | |\n| **spark.yarn.executor.memoryOverhead** | |\n\n## Impala\n[https://impala.apache.org/docs/build/html/topics/impala_query_options.html](https://impala.apache.org/docs/build/html/topics/impala_query_options.html)\n\n| 参数 |  |\n| ----- |  |\n| **sync_ddl** | | \n| **mem_limit** | |\n\n## MapReduce\n[https://hadoop.apache.org/docs/r3.0.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml](https://hadoop.apache.org/docs/r3.0.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)\n\n| 参数 |  |\n| ----- |  |\n| **mapreduce.map.java.opts** | |\n| **mapreduce.map.memory.mb** | |\n| **mapreduce.reduce.memory.mb** | |\n| **mapred.min.split.size** |  |\n| **mapred.min.split.size.per.node** |  |\n| **mapred.min.split.size.per.rack** |  |\n| **mapreduce.fileoutputcommitter.marksuccessfuljobs** |  |\n| **mapreduce.job.reduce.slowstart.completedmaps** |  |\n| **mapred.job.name** | |\n| **mapred.max.split.size** | |\n| **mapreduce.reduce.java.opts** | |\n\n## HDFS\n\n| 参数 |  |\n| ----- |  |\n| **dfs.datanode.max.xcievers** | | ","tags":["Impala"]}]